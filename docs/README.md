# Ninth: The Native Language of Neuro-Symbolic AI

![Version](https://img.shields.io/badge/version-v0.5.1-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-experimental-orange.svg)

**Ninth** is a minimal, Turing-complete, differentiable stack-based programming language designed for **Integrated Function Calling** within LLMs.

Unlike traditional tool use (JSON/Python), Ninth allows Large Language Models to "think" in code. It combines the simplicity of Forth with the power of PyTorch autograd.

> **"The model doesn't see the code execution, it only sees the result. It's like a Ghost in the Machine."**

---

## üöÄ Key Features

*   **Differentiable by Design**: Every operation supports Autograd. You can run `[BACKWARD]` through the entire execution stack.
*   **Tensor-First**: The only data type is `torch.Tensor`. Scalars, vectors, and matrices are first-class citizens.
*   **Token Efficient**: No boilerplate. `[MATMUL]` is 1 token. Loop logic is minimal. Ideal for limited context windows.
*   **Turing Complete**: Supports Functions (`[DEF]`), Loops (`[REPEAT]`), and Branching (`[IF]`).
*   **Smart Shapes (v0.5.1)**: Supports dynamic tensor creation like `[2 4] [RANDN]`.

---

## üì¶ Installation

Ninth is extremely lightweight. You only need `torch`.

```bash
pip install torch numpy matplotlib
```

Clone the repo and run the interpreter:

```bash
git clone https://github.com/your-username/ninth.git
cd ninth
python ninth.py
```

---

## ‚ö° Quick Start

### 1. Basic Math
Ninth uses RPN (Reverse Polish Notation).
```forth
[PROGRAM_START]
3 4 [ADD]     // Stack: 7
5 [MUL]       // Stack: 35
[PRINT]
[PROGRAM_END]
```

### 2. Autograd & Optimization
Ninth can calculate gradients and update variables. Here is how to find $\sqrt{16}$ using gradient descent:

```forth
[PROGRAM_START]
// Init x = 1.0 with gradients
1.0 [VAR] "x" [STORE]

// Training Loop (50 steps)
50 [REPEAT]
  "x" [LOAD] [DUP] [MUL]  // x^2
  16.0 [SUB] [DUP] [MUL]  // Loss = (x^2 - 16)^2
  
  [BACKWARD]              // Compute Gradients
  
  // SGD Step: x = x - 0.01 * grad
  "x" [LOAD] "x" [GRAD] 0.01 [MUL] [SUB] 
  [VAR] "x" [STORE]       // Update x
[END]

"Result:" [PRINT]
"x" [LOAD] [PEEK]         // Should be close to 4.0
[PROGRAM_END]
```

---

## üß† LLM Integration (System Prompt)

To make your LLM (GPT-4, Llama-3, Claude) use Ninth, inject this into the system prompt:

```text
You have access to a differentiable stack machine called "Ninth". 
To perform calculations, simulate logic, or optimize parameters, output code in a block:
[PROGRAM_START] ... code ... [PROGRAM_END]

Syntax:
- RPN: "3 4 [ADD]" -> 7
- Tensors: "[2 4] [RANDN]" -> 2x4 Matrix
- Variables: "val [VAR] 'name' [STORE]" and "'name' [LOAD]"
- Logic: "[IF] ... [ELSE] ... [ENDIF]"
- Loops: "N [REPEAT] ... [END]"
- Autograd: "[BACKWARD]" to compute gradients.
```

---

## üìä Neural Network Example

Ninth v0.5.1 is powerful enough to define and train a neural network from scratch in ~20 lines of code.


```forth
[PROGRAM_START]

// --- INIT ---
// W1: [784, 128]
[784 128] [RANDN] 0.01 [MUL] [VAR] "W1" [STORE]
[1 128] [ZEROS] [VAR] "b1" [STORE]

// W2: [128, 10]
[128 10] [RANDN] 0.01 [MUL] [VAR] "W2" [STORE]
[1 10] [ZEROS] [VAR] "b2" [STORE]

// Learning Rate
0.05 "lr" [STORE]

// --- FORWARD PASS ---
[DEF] "model"
   "W1" [LOAD] [MATMUL] "b1" [LOAD] [ADD] [RELU]
   "W2" [LOAD] [MATMUL] "b2" [LOAD] [ADD]
   // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–æ–≥–∏—Ç—ã (–±–µ–∑ Softmax, —Ç.–∫. CrossEntropy —Å–∞–º –µ–≥–æ —Å–¥–µ–ª–∞–µ—Ç –≤–Ω—É—Ç—Ä–∏)
[RET]

// --- TRAIN STEP ---
[DEF] "train_batch"
   // 1. –ó–∞–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—á–µ—Ä–µ–¥–∏ (Python –ø–æ–ª–æ–∂–∏–ª –∏—Ö —Ç—É–¥–∞)
   [INPUT] "X" [STORE]
   [INPUT] "Y" [STORE]

   // 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
   "X" [LOAD] [CALL] "model" // -> Logits

   // 3. –°—á–∏—Ç–∞–µ–º Loss
   [DUP] // –ö–æ–ø–∏—Ä—É–µ–º –ª–æ–≥–∏—Ç—ã (–æ–¥–Ω–∏ –¥–ª—è –ª–æ—Å—Å–∞, –¥—Ä—É–≥–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏)
   "Y" [LOAD] [CROSS_ENTROPY]
   "loss" [STORE]

   // 4. Backprop
   "loss" [LOAD] [BACKWARD]

   // 5. Update (SGD)
   "W1" [LOAD] "W1" [GRAD] "lr" [LOAD] [MUL] [SUB] [VAR] "W1" [STORE]
   "b1" [LOAD] "b1" [GRAD] "lr" [LOAD] [MUL] [SUB] [VAR] "b1" [STORE]
   "W2" [LOAD] "W2" [GRAD] "lr" [LOAD] [MUL] [SUB] [VAR] "W2" [STORE]
   "b2" [LOAD] "b2" [GRAD] "lr" [LOAD] [MUL] [SUB] [VAR] "b2" [STORE]
   
   // 6. –í—ã–≤–æ–¥ —Ç–µ–∫—É—â–µ–≥–æ –ª–æ—Å—Å–∞ (–¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã)
   "loss" [LOAD] [PRINT]
[RET]

[PROGRAM_END]

```
---

## üó∫ Roadmap

- [x] **v0.1**: Basic Stack & Math
- [x] **v0.3**: Loops & Control Flow
- [x] **v0.5**: Smart Shapes & Stability
- [ ] **v0.6**: File I/O & Tensor Reshape Support
- [ ] **v0.7**: Standard Library
- [ ] **v1.0**: End-to-End LLM Fine-tuning Integration

## üìÑ License

MIT License. Free to use for research and revolution.

## README IS AI GENERATED BY GEMINI 3.0 PRO
```
